# Logs Directory

This directory serves as the central repository for all system logs, observability data, and execution tracking information generated by the AdvancedRules framework. It provides comprehensive visibility into system operations, performance metrics, and troubleshooting data.

## ðŸ“ Directory Structure

```
logs/
â”œâ”€â”€ observability/              # Real-time monitoring and metrics
â”‚   â”œâ”€â”€ summary.md             # Executive summary and key metrics
â”‚   â”œâ”€â”€ summary.json           # Structured metrics data
â”‚   â””â”€â”€ [additional metrics files]
â”œâ”€â”€ decisions/                  # Decision tracking and analysis
â”‚   â””â”€â”€ [decision log files]
â”œâ”€â”€ events.jsonl               # Event stream data
â”œâ”€â”€ decision_metrics.json      # Decision performance metrics
â””â”€â”€ [additional log files]     # Framework-generated log files
```

## ðŸ“Š Log Categories

### Observability Logs (`observability/`)
**Real-time system monitoring and performance data**

#### `summary.md`
- **Purpose**: Executive-level summary of system status and key metrics
- **Content**: Performance indicators, quality scores, and operational status
- **Format**: Markdown with structured sections and visual indicators
- **Update Frequency**: Real-time updates during execution

#### `summary.json`
- **Purpose**: Structured metrics data for programmatic access and analysis
- **Content**: JSON-formatted metrics with timestamps and metadata
- **Format**: Standardized JSON schema with version control
- **Usage**: Integration with monitoring dashboards and alerting systems

### Decision Logs (`decisions/`)
**Tracking and analysis of AI decision-making processes**

#### Decision Tracking
- **Decision Records**: Complete record of all decisions made by AI personas
- **Context Information**: Input data, constraints, and environmental factors
- **Rationale Documentation**: Explanation of decision logic and reasoning
- **Outcome Tracking**: Results and impact of decisions made

#### Performance Analysis
- **Decision Quality**: Accuracy and effectiveness of decisions
- **Processing Time**: Time taken to make decisions
- **Confidence Levels**: AI confidence in decision recommendations
- **Success Correlation**: Correlation between decisions and project outcomes

### Event Stream (`events.jsonl`)
**Real-time event streaming and activity tracking**

#### Event Types
- **System Events**: Framework startup, shutdown, configuration changes
- **Persona Events**: AI persona activation, execution, completion
- **Workflow Events**: State transitions, quality gate validations
- **Error Events**: Exceptions, failures, and recovery actions

#### Event Structure
```json
{
  "timestamp": "2024-12-01T14:30:00Z",
  "event_type": "persona_execution",
  "persona": "product_owner_ai",
  "action": "requirements_analysis",
  "status": "completed",
  "duration_ms": 1250,
  "metadata": {
    "project_id": "proj_123",
    "quality_score": 0.92,
    "artifacts_generated": 3
  }
}
```

### Metrics Logs (`decision_metrics.json`)
**Quantitative performance and quality metrics**

#### Metrics Categories
- **Quality Metrics**: Code quality, security compliance, test coverage
- **Performance Metrics**: Response times, throughput, resource utilization
- **Success Metrics**: Project completion rates, client satisfaction
- **Efficiency Metrics**: Time savings, cost reductions, resource optimization

## ðŸŽ¯ Log Management

### Log Rotation
```bash
# Log rotation configuration
log_rotation:
  max_file_size: 100MB
  max_files: 10
  compression: gzip
  retention_days: 90

# Automatic cleanup
find logs/ -name "*.log" -mtime +90 -delete
find logs/ -name "*.gz" -mtime +365 -delete
```

### Log Levels
```python
# Logging level configuration
logging:
  level: INFO  # DEBUG, INFO, WARNING, ERROR, CRITICAL
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  handlers:
    - file: logs/framework.log
    - console: true
    - json: logs/events.jsonl
```

### Structured Logging
```python
import logging
import json
from datetime import datetime

class StructuredLogger:
    def __init__(self, name):
        self.logger = logging.getLogger(name)

    def log_execution_event(self, persona, action, status, **kwargs):
        event = {
            "timestamp": datetime.utcnow().isoformat(),
            "event_type": "persona_execution",
            "persona": persona,
            "action": action,
            "status": status,
            **kwargs
        }
        self.logger.info(json.dumps(event))
```

## ðŸ“ˆ Analytics & Reporting

### Real-time Dashboards
```python
# Dashboard integration
def generate_dashboard_data():
    """Generate real-time dashboard metrics"""
    metrics = {
        "active_personas": count_active_personas(),
        "quality_score": calculate_average_quality(),
        "execution_time": measure_execution_time(),
        "error_rate": calculate_error_rate(),
        "throughput": measure_system_throughput()
    }
    return metrics
```

### Performance Analytics
```python
# Performance analysis functions
def analyze_execution_performance():
    """Analyze execution performance from logs"""
    executions = parse_execution_logs()
    return {
        "average_duration": calculate_average_duration(executions),
        "success_rate": calculate_success_rate(executions),
        "bottlenecks": identify_bottlenecks(executions),
        "optimization_opportunities": find_optimization_opportunities(executions)
    }
```

### Quality Analytics
```python
# Quality metrics analysis
def analyze_quality_metrics():
    """Analyze quality metrics from logs"""
    quality_data = parse_quality_logs()
    return {
        "average_quality_score": calculate_average_score(quality_data),
        "quality_trends": analyze_quality_trends(quality_data),
        "improvement_areas": identify_improvement_areas(quality_data),
        "compliance_status": check_compliance_status(quality_data)
    }
```

## ðŸ” Log Analysis Tools

### Log Querying
```bash
# Query logs using command-line tools
# Search for specific events
grep "persona_execution" logs/events.jsonl

# Filter by time range
awk '$1 >= "2024-12-01" && $1 <= "2024-12-31"' logs/events.jsonl

# Extract specific fields
jq '.persona' logs/events.jsonl | sort | uniq -c
```

### Log Visualization
```python
# Log visualization tools
import matplotlib.pyplot as plt
import pandas as pd

def visualize_execution_trends():
    """Create execution trend visualizations"""
    df = pd.read_json('logs/events.jsonl', lines=True)

    # Execution duration over time
    plt.figure(figsize=(12, 6))
    plt.plot(df['timestamp'], df['duration_ms'])
    plt.title('Execution Duration Trends')
    plt.xlabel('Time')
    plt.ylabel('Duration (ms)')
    plt.show()
```

### Automated Reporting
```python
# Automated report generation
def generate_executive_report():
    """Generate executive-level summary report"""
    report_data = {
        "period": get_reporting_period(),
        "key_metrics": collect_key_metrics(),
        "quality_scores": analyze_quality_scores(),
        "performance_indicators": calculate_performance_indicators(),
        "recommendations": generate_recommendations()
    }

    # Generate markdown report
    report = generate_markdown_report(report_data)
    save_report(report, f"logs/reports/executive_{get_timestamp()}.md")

    return report
```

## ðŸ”’ Security & Compliance

### Log Security
- **Access Control**: Restricted access to sensitive log information
- **Data Encryption**: Encryption of logs containing sensitive data
- **Audit Logging**: Logs of all log access and modifications
- **Retention Policies**: Compliance with data retention requirements

### Compliance Logging
```yaml
# Compliance logging configuration
compliance_logging:
  gdpr_compliance: enabled
  data_retention: 7_years
  audit_trail: enabled
  access_logging: enabled

  sensitive_data_handling:
    mask_pii: true
    encrypt_sensitive: true
    retention_limits: strict
```

## ðŸ“Š Monitoring Integration

### Alerting System
```python
# Alert configuration
alerts:
  error_rate_threshold: 5%
  quality_score_threshold: 0.80
  execution_timeout_threshold: 300000  # 5 minutes
  resource_usage_threshold: 90%

def check_alerts():
    """Check for alert conditions"""
    metrics = get_current_metrics()

    alerts = []
    if metrics['error_rate'] > alerts['error_rate_threshold']:
        alerts.append("High error rate detected")

    if metrics['quality_score'] < alerts['quality_score_threshold']:
        alerts.append("Quality score below threshold")

    return alerts
```

### Metrics Collection
```python
# Metrics collection configuration
metrics_collection:
  interval: 30  # seconds
  retention: 90  # days
  aggregation:
    hourly: enabled
    daily: enabled
    weekly: enabled

  exporters:
    - prometheus
    - cloudwatch
    - custom_dashboard
```

## ðŸš€ Best Practices

### Log Management
- **Consistent Formatting**: Use consistent log formats across all components
- **Structured Data**: Include structured data for easy parsing and analysis
- **Context Information**: Include relevant context in all log entries
- **Performance Impact**: Minimize logging performance impact on production systems

### Monitoring Guidelines
- **Key Metrics**: Focus on actionable metrics that drive decisions
- **Alert Fatigue**: Avoid excessive alerting that leads to alert fatigue
- **Data Quality**: Ensure log data quality and completeness
- **Cost Optimization**: Balance logging detail with storage and processing costs

### Analysis Approaches
- **Automated Analysis**: Implement automated log analysis and anomaly detection
- **Trend Analysis**: Track trends over time for proactive issue identification
- **Root Cause Analysis**: Use logs for effective troubleshooting and RCA
- **Continuous Improvement**: Use log insights to drive system improvements

---

## ðŸ“š Related Documentation

- **[Framework Overview](../README.md)**: Main project documentation
- **[Observability Tools](../tools/observability/)**: Monitoring and observability toolkit
- **[Security Guide](../docs/security/)**: Security logging and compliance
- **[Troubleshooting Guide](../docs/troubleshooting/)**: Using logs for debugging

---

*Logs Directory - Comprehensive system observability and operational intelligence.*
